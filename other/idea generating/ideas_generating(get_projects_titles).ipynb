{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8trQSt1a1x3q"
   },
   "source": [
    "**This piece of code is part of an on-going project to generate ideas for projects in the field of artificial intelligence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw3EdTQe2Z9g"
   },
   "source": [
    "In this code we gather data of projects titles from several URLs then preprocess the data to be used in text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxbJQ34E_Fl3"
   },
   "outputs": [],
   "source": [
    "# scrape past stanford projects titles\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "result = requests.get(\"http://cs229.stanford.edu/projects.html?fbclid=IwAR1GZEZmRqaESnYsqG1te5MUiCL5mBLe_DdVvJXFQhEBjR3OmmjaFHMF3NA\")\n",
    "c = result.content\n",
    "soup = BeautifulSoup(c)\n",
    "pattern = re.compile(r'Previous projects')\n",
    "linksSec = soup.find('h3', text=pattern).find_next_sibling('div').findAll('a',href=True)\n",
    "links = []\n",
    "for link in linksSec[:7]:\n",
    "  links.append('http://cs229.stanford.edu/'+link['href'])\n",
    "projects = []\n",
    "for link in links[:-2]:\n",
    "  result = requests.get(link)\n",
    "  c = result.content\n",
    "  soup = BeautifulSoup(c)\n",
    "  projects.extend(soup.find_all(\"p\", {\"class\": \"project-title\"},text=True))\n",
    "for link in links[-2:]:\n",
    "  result = requests.get(link)\n",
    "  c = result.content\n",
    "  soup = BeautifulSoup(c)\n",
    "  projects.extend(soup.find_all(\"b\",text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_EB0ywR3kLj"
   },
   "source": [
    "Save data to ideas.txt file\n",
    "\n",
    "Notice: this file includes other data gathered from several URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxx_S-VaCdiE"
   },
   "outputs": [],
   "source": [
    "file_object = open('ideas.txt', 'a')\n",
    "for project in projects:\n",
    "  file_object.write(project.text + '\\n')\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKIs9bX83qeC"
   },
   "source": [
    "Some text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncWGYWrDtfqv"
   },
   "outputs": [],
   "source": [
    "#!pip3 install contractions\n",
    "import contractions\n",
    "def expand_contractions(text):\n",
    "    text = contractions.fix(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N14RNJwktdr7"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(\"ideas.txt\",'r')\n",
    "Lines = file1.readlines()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "new_doc = ''\n",
    "for line in Lines:\n",
    "  words = line.split()\n",
    "  new_line = ''\n",
    "  for r in words:\n",
    "    if not r in stop_words:\n",
    "      new_line = new_line + ' ' + lemmatizer.lemmatize(r.lower())\n",
    "  \n",
    "  appendFile = open('filteredideas.txt','a')\n",
    "  appendFile.write(expand_contractions(new_line.strip()+'\\n'))\n",
    "  appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbCBvDFeEYLh"
   },
   "outputs": [],
   "source": [
    "!pip3 install git+git://github.com/minimaxir/textgenrnn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q71gmitv2E1"
   },
   "source": [
    "**Using pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0C5efC01WCr"
   },
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "textgen = textgenrnn()\n",
    "textgen.train_from_file('filteredideas.txt', num_epochs=30)\n",
    "textgen.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for me:\n",
    "next remove punctuations but try to leave stopwords or at least leave some of them**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ideas generating(get_projects_titles).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
